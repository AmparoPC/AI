{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semana1_Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq7utTGXu7oO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBvxRn_luY2k"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json\n",
        "  \n",
        "import json  #permite descargar la informacion en formato json y estructurarlo en formato python\n",
        "\n",
        "with open(\"/tmp/sarcasm.json\", 'r') as f: \n",
        "    datastore = json.load(f)     #carga el codigo de kaggle que permite saber si un articulo es sarcástico o no\n",
        "\n",
        "\n",
        "sentences = [] \n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "    urls.append(item['article_link'])\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer #API tokenizer permite dar valores a las palabras, no distingue mayus/minus ni signos puntuacion\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences #para ajustar el tamaño de las frases\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\") #añade 'out of vocabulary' cuando la palabra no ha sido vista anteriormente\n",
        "tokenizer.fit_on_texts(sentences)  #accion del tokenizer\n",
        "\n",
        "word_index = tokenizer.word_index #devuelve lista valores de palabra y código\n",
        "print(len(word_index)) \n",
        "print(word_index)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)  #transforma las frases en secuencias de numeros\n",
        "padded = pad_sequences(sequences, padding='post')  #ajusta el tamaño para que todas sean del mismo tamaño\n",
        "print(padded[0]) #imprime la primera frase\n",
        "print(padded.shape) # filas columnas matriz (numero de frases y su longitud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI5pJXwyuZr9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}